1
00:00:00,590 --> 00:00:02,070
This lecture's about two things.

2
00:00:02,070 --> 00:00:04,353
First it's about predicting with
regression and using

3
00:00:04,353 --> 00:00:07,168
multiple covariates, but more importantly
it's about exploring a

4
00:00:07,168 --> 00:00:09,302
data set and trying to identify which
predictors are

5
00:00:09,302 --> 00:00:11,600
the most important to include in our
prediction model.

6
00:00:13,490 --> 00:00:15,818
So we're again going to be using the wages
data to try to

7
00:00:15,818 --> 00:00:19,340
predict the wages of a group of men that
come from the mid Atlantic.

8
00:00:19,340 --> 00:00:24,270
This data set is available at the ISLR
package which you can find here.

9
00:00:24,270 --> 00:00:26,830
And it comes from the book Introduction to
statistical learning.

10
00:00:28,320 --> 00:00:30,380
So the first thing that we do is that we
load

11
00:00:30,380 --> 00:00:32,530
the data set in, and so here is the ISLR
data set.

12
00:00:32,530 --> 00:00:36,590
We're also loading the ggplot2 package for
some

13
00:00:36,590 --> 00:00:39,300
exploratory analysis, and we're loading
the caret package.

14
00:00:39,300 --> 00:00:41,700
We're doing prediction, and we're going to

15
00:00:41,700 --> 00:00:44,490
subset the data set for exploration
purposes to

16
00:00:44,490 --> 00:00:47,120
just the part of the data set that isn't
the variable we're trying to predict.

17
00:00:47,120 --> 00:00:49,400
So we select out the log wage variable,
which is

18
00:00:49,400 --> 00:00:55,020
the variable that we're going to be
predicting in this analysis.

19
00:00:55,020 --> 00:00:57,233
Then we can look at a summary of the data
set and

20
00:00:57,233 --> 00:01:00,495
so we can already see some features, for
example, we saw before,

21
00:01:00,495 --> 00:01:03,235
that this data set has only males included
in the data set

22
00:01:03,235 --> 00:01:07,170
and that for example the region is
entirely peopled from the mid Atlantic.

23
00:01:09,130 --> 00:01:11,710
So to do a little bit more exploration, we
first

24
00:01:11,710 --> 00:01:15,450
need to train subset things into the
training and test sets.

25
00:01:15,450 --> 00:01:18,400
So we use that with the Create Data
Partition function.

26
00:01:18,400 --> 00:01:20,600
And we subset into a training and test
set.

27
00:01:20,600 --> 00:01:24,350
And so we're going to do all of our
exploration again on the training set.

28
00:01:24,350 --> 00:01:27,680
Because when we're building models, we're
not going to use any of the test set.

29
00:01:27,680 --> 00:01:28,750
We're only going to apply it once.

30
00:01:30,070 --> 00:01:32,620
So the first thing that you can do is a
feature plot.

31
00:01:32,620 --> 00:01:34,548
So this feature plot says, shows a little
bit

32
00:01:34,548 --> 00:01:37,400
about how the variables are related to
each other.

33
00:01:37,400 --> 00:01:40,590
Sometimes this plot is useful and
sometimes it isn't.

34
00:01:40,590 --> 00:01:42,910
In this particular plot it's a little bit
hard

35
00:01:42,910 --> 00:01:45,310
to see because everything is so squished
together but if

36
00:01:45,310 --> 00:01:49,140
you make it on your own, by using this
function

37
00:01:49,140 --> 00:01:51,910
you'll see that it's a little bit easier
to see.

38
00:01:51,910 --> 00:01:54,387
So, for example, you can see that there
appears

39
00:01:54,387 --> 00:01:56,864
for the job class there appears to be this
group,

40
00:01:56,864 --> 00:01:58,980
appears to be a little bit higher than
that

41
00:01:58,980 --> 00:02:01,960
group for the job class in terms of the
outcome.

42
00:02:01,960 --> 00:02:03,998
And you can see that there is, at least

43
00:02:03,998 --> 00:02:07,680
for this age variable the relationship to
the outcome.

44
00:02:07,680 --> 00:02:11,869
Again there seems to be some outlier
group, two separate groups here that gives

45
00:02:11,869 --> 00:02:15,920
us some indication that we might be able
to use that variable to predict.

46
00:02:15,920 --> 00:02:20,850
So the first thing that we can do is plot
the variables versus weight age.

47
00:02:20,850 --> 00:02:23,843
So this is plotting the wage variable
versus the age

48
00:02:23,843 --> 00:02:26,628
variable and you can see that there
appears to be

49
00:02:26,628 --> 00:02:29,272
some kind of trend which we saw in the
previous

50
00:02:29,272 --> 00:02:32,474
lectures, but there also is this set of
points that's

51
00:02:32,474 --> 00:02:35,117
outlined up here, and so the idea is that
that

52
00:02:35,117 --> 00:02:38,946
set of points that's outlined up there
might be, something

53
00:02:38,946 --> 00:02:41,730
that we can predict, but we would have to
figure

54
00:02:41,730 --> 00:02:45,530
out what variable it is that, is
representing that chunk.

55
00:02:46,560 --> 00:02:48,448
So, one way that you can do that is, you

56
00:02:48,448 --> 00:02:52,560
plot one variable, one predictor, age
versus the outcome, wage.

57
00:02:52,560 --> 00:02:57,290
And then you color the points by another
variable, in this case job class.

58
00:02:57,290 --> 00:03:00,788
And you can see for example that it
appears that most of these points up

59
00:03:00,788 --> 00:03:02,536
here are blue instead of pink and so

60
00:03:02,536 --> 00:03:05,960
that means they come from the information
group.

61
00:03:05,960 --> 00:03:07,780
So, this gives us some indication that the

62
00:03:07,780 --> 00:03:10,180
information variable might be able to
predict at least

63
00:03:10,180 --> 00:03:14,450
some fraction of the variability that's in
that top class up at the top of the plot.

64
00:03:15,790 --> 00:03:17,480
You can also color it by education.

65
00:03:17,480 --> 00:03:21,140
So, here I'm just doing a q plot again.

66
00:03:21,140 --> 00:03:23,710
So I'm plotting age versus wage.

67
00:03:23,710 --> 00:03:26,260
And I'm telling it to color the plot by
education.

68
00:03:26,260 --> 00:03:29,547
And, again, I'm only using the training
set because we're trying

69
00:03:29,547 --> 00:03:33,830
to, only look at training set for our
development of our model.

70
00:03:33,830 --> 00:03:37,720
And you can see that the advance degree,
also explains,

71
00:03:37,720 --> 00:03:40,990
a lot of the variation up here in the top
group.

72
00:03:40,990 --> 00:03:44,250
And so, some combination of, degree, and,

73
00:03:45,350 --> 00:03:49,300
class of job, could explain why, the
relationship

74
00:03:49,300 --> 00:03:54,170
between age and wage isn't just a perfect
relationship down here, in one big cloud.

75
00:03:56,150 --> 00:03:59,800
So the next thing that we can do is fit a
linear model with multiple variables in it

76
00:03:59,800 --> 00:04:01,440
and so the idea here is, again, we're just

77
00:04:01,440 --> 00:04:04,360
fitting lines, but now we're fitting more
than one line.

78
00:04:04,360 --> 00:04:08,262
And so the idea is, we have some intercept
terms, so that's

79
00:04:08,262 --> 00:04:12,853
just the baseline level of, wage that we
might have, and then

80
00:04:12,853 --> 00:04:16,677
we might have a, a relationship with the
age of the person,

81
00:04:16,677 --> 00:04:21,850
and then we might have relationship with
what job class you're in.

82
00:04:21,850 --> 00:04:27,080
So one way that we typically do that is,
by fitting an indicator variable.

83
00:04:27,080 --> 00:04:29,020
So an indicator variable is a variable

84
00:04:29,020 --> 00:04:31,240
that's denoted like this in mathematical
notation.

85
00:04:31,240 --> 00:04:33,322
It just says, if the job class for the ith

86
00:04:33,322 --> 00:04:37,360
person is equal to information, this
variable's equal to one.

87
00:04:37,360 --> 00:04:41,065
If the job class for the ith person is not
equal to information, then

88
00:04:41,065 --> 00:04:45,680
this information is equal to zero, and so
this represents the difference in the

89
00:04:45,680 --> 00:04:47,825
wages between the people with job class

90
00:04:47,825 --> 00:04:50,230
equal to information versus job class
equal

91
00:04:50,230 --> 00:04:55,450
to not information, when you, fix all the
other variables in the regression model.

92
00:04:55,450 --> 00:04:58,560
You can also do this for education it's a
little

93
00:04:58,560 --> 00:05:01,940
bit more complicated road because there
are multiple education levels.

94
00:05:01,940 --> 00:05:06,695
So we create an indicator variable for,
each of the different education

95
00:05:06,695 --> 00:05:11,520
levels and so, here this is the sum of
four indicator variables.

96
00:05:11,520 --> 00:05:17,620
And so the, the variable's equal to one,
if the education

97
00:05:17,620 --> 00:05:22,950
for person I is equal to level K, that
variables equal to one and zero otherwise.

98
00:05:24,660 --> 00:05:26,838
So then we can fit the model just like we
did before

99
00:05:26,838 --> 00:05:30,430
so we have, we're using the train function
in the caret package.

100
00:05:30,430 --> 00:05:34,560
And we again use wage as the outcome and
then tilde represents the formula

101
00:05:34,560 --> 00:05:37,770
on the right, it's going to be used to
predict the variable on the left.

102
00:05:37,770 --> 00:05:41,040
So we use age, job class, and education.

103
00:05:41,040 --> 00:05:44,570
So job class and education are both factor
variables in r and so by

104
00:05:44,570 --> 00:05:46,880
default it does, you know, it creates

105
00:05:46,880 --> 00:05:49,800
these indicator variables like I've shown
here.

106
00:05:49,800 --> 00:05:55,340
So, that when it fits the model it takes
that into account by automatically.

107
00:05:55,340 --> 00:05:56,870
Again we're fitting the model on the
training set

108
00:05:56,870 --> 00:05:59,220
and we can look at the final model and

109
00:05:59,220 --> 00:06:00,900
you can see now that the final model has

110
00:06:00,900 --> 00:06:04,290
ten predictors even though we only put
three variable

111
00:06:04,290 --> 00:06:07,240
names into the formula and the reason is
because

112
00:06:07,240 --> 00:06:11,910
this, variable right here, got, actually
received more than

113
00:06:11,910 --> 00:06:14,160
one, predictor in the data set because of
the

114
00:06:14,160 --> 00:06:16,530
way that we had to create these indicator
functions.

115
00:06:19,560 --> 00:06:21,380
So then we can look at some diagnostic
plots.

116
00:06:21,380 --> 00:06:24,420
So this is very typical for when you're
building these regression models.

117
00:06:24,420 --> 00:06:26,309
The idea here is you can plot the fitted

118
00:06:26,309 --> 00:06:28,909
values, so this is the predictions from
our model on

119
00:06:28,909 --> 00:06:32,100
the training set versus the residuals,
that's the amount

120
00:06:32,100 --> 00:06:35,590
of variation that's left over after you
fit your model.

121
00:06:35,590 --> 00:06:37,360
And so what you'd like to see is that.

122
00:06:37,360 --> 00:06:42,795
This line would be centered at zero on
this, axis because the residuals is the,

123
00:06:42,795 --> 00:06:46,200
difference between our model prediction
and our,

124
00:06:46,200 --> 00:06:50,040
actual real values that we're trying to
predict.

125
00:06:50,040 --> 00:06:52,359
And here, you can see there's still a
couple of outliers

126
00:06:52,359 --> 00:06:54,450
up here that have been labeled for you in
this plot.

127
00:06:55,602 --> 00:06:58,443
And so, those variables might be variables
that we

128
00:06:58,443 --> 00:07:00,739
want to try to explore a little bit
further and

129
00:07:00,739 --> 00:07:03,278
see if we can identify any other
predictors in

130
00:07:03,278 --> 00:07:05,710
our data set that might be able to explain
them.

131
00:07:07,440 --> 00:07:10,530
The other thing that we can do is color by
variables not just used in the model.

132
00:07:10,530 --> 00:07:12,646
So for example here I'm plotting again the

133
00:07:12,646 --> 00:07:15,980
fitted values from our model versus the
residuals.

134
00:07:15,980 --> 00:07:17,980
And so again, we like to see this laying
on the zero

135
00:07:17,980 --> 00:07:22,402
line because it's the difference between
our fitted values and our real values.

136
00:07:22,402 --> 00:07:25,826
And so, what we can do is plot on this
plot, we can again plot

137
00:07:25,826 --> 00:07:30,429
it by different variables, in this case,
we can plot it by for example, race.

138
00:07:30,429 --> 00:07:33,109
And so you can see that it seems like some
of these

139
00:07:33,109 --> 00:07:36,459
outliers up here may be explained by the
race variable in the

140
00:07:36,459 --> 00:07:40,881
data set and so these another exploratory
technique plotting the fitted model

141
00:07:40,881 --> 00:07:43,025
versus the residuals then coloring it

142
00:07:43,025 --> 00:07:46,190
by different variables to identify
potential trends.

143
00:07:47,270 --> 00:07:49,662
Another thing that can be really useful

144
00:07:49,662 --> 00:07:53,830
is plotting the fitted residuals versus
the index.

145
00:07:53,830 --> 00:07:54,910
And what do I mean by the index?

146
00:07:54,910 --> 00:08:00,530
So the data set comes in a set of rows
that you got in a particular order.

147
00:08:00,530 --> 00:08:03,900
And so the index is just which row of the
data set you're looking at.

148
00:08:04,970 --> 00:08:08,480
And the y axis here is the residuals.

149
00:08:08,480 --> 00:08:12,410
And so you can see that all the residuals
seems to be happening, high residuals seem

150
00:08:12,410 --> 00:08:18,450
to be happening down here at the right end
of the high, the highest row numbers.

151
00:08:18,450 --> 00:08:21,657
And you can also see a trend with respect
to row numbers and so

152
00:08:21,657 --> 00:08:25,571
whenever you can see a trend or a outlier
like that with respect to the

153
00:08:25,571 --> 00:08:27,818
row numbers, it suggests that there's a

154
00:08:27,818 --> 00:08:30,517
variable missing from your model because
you're

155
00:08:30,517 --> 00:08:32,956
plotting the residuals here, so that's the

156
00:08:32,956 --> 00:08:35,810
difference between the true values and the
fitted.

157
00:08:35,810 --> 00:08:38,300
And that shouldn't have any relationship
to the order

158
00:08:38,300 --> 00:08:40,880
in which the variables appear in your data
set.

159
00:08:40,880 --> 00:08:44,190
Unless, and this is what's typically you
discover when you see

160
00:08:44,190 --> 00:08:46,885
a trend like this, or outliers like this
at one end of

161
00:08:46,885 --> 00:08:50,315
this plot, that there's a relationship
with respect to time, or

162
00:08:50,315 --> 00:08:53,960
age, or some other continuous variable
that the rows are ordered by.

163
00:08:56,170 --> 00:09:00,880
So the other thing that you can do is plot
the wage variable.

164
00:09:00,880 --> 00:09:03,070
So this is the wage variable in the test

165
00:09:03,070 --> 00:09:06,840
set versus the predicted values in the
test set.

166
00:09:06,840 --> 00:09:09,870
So ideally these two things would be very
close to each other.

167
00:09:09,870 --> 00:09:13,501
Ideally you'd have essentially a straight
line on the 45

168
00:09:13,501 --> 00:09:17,290
degree line where wage was exactly equal
to our predictions.

169
00:09:17,290 --> 00:09:20,080
Of course that isn't how it always works
out.

170
00:09:20,080 --> 00:09:22,400
And then in the test set, you can explore

171
00:09:22,400 --> 00:09:24,580
and try to identify trends that you might
have missed.

172
00:09:24,580 --> 00:09:27,490
So, for example, here we're looking at the
year

173
00:09:27,490 --> 00:09:30,030
that the data was collected in the test
set.

174
00:09:30,030 --> 00:09:33,170
As a way of exploring how our model might
have broken down.

175
00:09:33,170 --> 00:09:34,710
Now something to keep in mind is that if
you do

176
00:09:34,710 --> 00:09:37,450
this sort of exploration in the test set,
you can't then

177
00:09:37,450 --> 00:09:40,490
go back and re-update your model in the
training set because

178
00:09:40,490 --> 00:09:43,500
that would be using the test set to
rebuild your predictors.

179
00:09:43,500 --> 00:09:46,976
This is more like a post-mortem on your
analysis or a

180
00:09:46,976 --> 00:09:51,010
way to try to determine whether your
analysis worked or not.

181
00:09:53,290 --> 00:09:56,243
If you want all of the covariants in your
model building, one

182
00:09:56,243 --> 00:09:59,370
thing that you can do is use this again in
the training function.

183
00:09:59,370 --> 00:10:03,080
You can pass it an outcome and then tilde
and then if you put

184
00:10:03,080 --> 00:10:08,140
a dot here instead of putting a set of
variables separated by plus signs.

185
00:10:08,140 --> 00:10:11,320
It says predict with all of the variables
in the data set.

186
00:10:11,320 --> 00:10:15,270
So, this is model fit with all of the
variables and

187
00:10:15,270 --> 00:10:18,590
so this is the wage variable and the
predictions here and

188
00:10:18,590 --> 00:10:20,590
so you can actually see that it does a
little bit

189
00:10:20,590 --> 00:10:23,650
better when you include all of the
variables in the data set.

190
00:10:23,650 --> 00:10:26,100
This is By default if you don't want to

191
00:10:26,100 --> 00:10:28,070
try to do some sort of model selection in
advance.

192
00:10:30,040 --> 00:10:33,744
So linear regression is often useful in
combination with other models.

193
00:10:33,744 --> 00:10:35,528
It's a quite a simple model in the

194
00:10:35,528 --> 00:10:38,490
sense that it always fits lines through
the data.

195
00:10:38,490 --> 00:10:41,347
And so it can be, capture a lot of
variability

196
00:10:41,347 --> 00:10:45,860
if the relationship between the predictors
and the outcome is linear.

197
00:10:45,860 --> 00:10:47,620
If it's not linear, then you can often
miss

198
00:10:47,620 --> 00:10:50,380
things, and it can be better blended with
other models.

199
00:10:50,380 --> 00:10:53,140
We'll talk about model blending later in
the class.

200
00:10:53,140 --> 00:10:55,860
Exploratory data analysis can be very
useful with regression

201
00:10:55,860 --> 00:10:58,480
models because, like, the plots we made
with residuals

202
00:10:58,480 --> 00:11:01,410
and so forth colored by different
features, you can

203
00:11:01,410 --> 00:11:03,130
try to identify the patterns in the data
set.

204
00:11:04,600 --> 00:11:06,940
There's more information in these three
books, if you want to

205
00:11:06,940 --> 00:11:09,920
go and find a lot more about regression
modeling for prediction.

