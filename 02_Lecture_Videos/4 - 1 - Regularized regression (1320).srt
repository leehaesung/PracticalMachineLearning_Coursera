1
00:00:00,150 --> 00:00:01,760
This lecture is about Regularized
regression.

2
00:00:01,760 --> 00:00:05,730
We learned about linear regression and
generalized linear regression previously.

3
00:00:05,730 --> 00:00:08,984
For the basic idea here is to fit one of
these regression models.

4
00:00:08,984 --> 00:00:10,832
And then, penalize or shrink the large

5
00:00:10,832 --> 00:00:14,090
coefficients corresponding with some of
the predictor variables.

6
00:00:14,090 --> 00:00:15,699
The reason why we might do this is

7
00:00:15,699 --> 00:00:19,140
because it might help with the bias
variance tradeoff.

8
00:00:19,140 --> 00:00:21,760
If certain variables are highly
coordinated with each other.

9
00:00:21,760 --> 00:00:24,152
For example, you might not want to include
them both in

10
00:00:24,152 --> 00:00:27,880
the linear regression model as they will
have a very high variance.

11
00:00:27,880 --> 00:00:31,700
Which might slightly, leaving one of them
out, might slightly bias your model.

12
00:00:31,700 --> 00:00:34,915
In other words, you might lose a little
bit of prediction capability, but

13
00:00:34,915 --> 00:00:36,344
you'll save a lot on the variants

14
00:00:36,344 --> 00:00:39,300
and therefore improve your prediction to
error.

15
00:00:39,300 --> 00:00:41,320
It can also help with model selection, in

16
00:00:41,320 --> 00:00:44,700
certain cases for regular organization
techniques like the lasso.

17
00:00:46,170 --> 00:00:48,543
It may be computationally demanding on
large data sets.

18
00:00:48,543 --> 00:00:51,299
And in general, it appears that it does
not perform as quite as

19
00:00:51,299 --> 00:00:54,760
well as random forests or boosting, when
applied to prediction in the wild.

20
00:00:54,760 --> 00:00:57,363
For example, in cattle competitions.

21
00:00:57,363 --> 00:01:00,930
So as a motivating example, suppose we fit
a very simple regression model.

22
00:01:00,930 --> 00:01:02,670
So there's an outcome Y.

23
00:01:02,670 --> 00:01:05,580
And we're trying to predict it with two
covariants, x1 and x2.

24
00:01:05,580 --> 00:01:08,780
So we have an intercept terms so this is a
constant.

25
00:01:08,780 --> 00:01:13,800
Plus another constant times x1 plus
another constant times x2.

26
00:01:13,800 --> 00:01:16,770
>> So assume that x1 and x2 are nearly
perfectly correlated.

27
00:01:16,770 --> 00:01:19,660
In other words they're almost exactly the
same variable.

28
00:01:20,770 --> 00:01:25,400
The word for this in linear modeling is
often called co-linear.

29
00:01:25,400 --> 00:01:28,590
You can then approximate this more
complicated model.

30
00:01:28,590 --> 00:01:31,508
By saying, well, what if we just include
only X1

31
00:01:31,508 --> 00:01:34,960
and multiply it by both the coefficients
for X1 and X2.

32
00:01:34,960 --> 00:01:40,800
It won't be exactly right, because X1 and
X2 aren't exactly the same variable.

33
00:01:40,800 --> 00:01:44,540
But it will be very close to right, if X1
and X2 are very similar to each other.

34
00:01:44,540 --> 00:01:47,934
The result may be that you still get a
very good estimate of y.

35
00:01:47,934 --> 00:01:50,114
The same, almost as good as you would've

36
00:01:50,114 --> 00:01:53,540
got by including both predictors in the
model.

37
00:01:53,540 --> 00:01:55,350
And you, it will be a little bit biased,

38
00:01:55,350 --> 00:01:57,870
because we chose to leave one of the
predictors out.

39
00:01:57,870 --> 00:01:59,910
But we may reduce the variance if those

40
00:01:59,910 --> 00:02:01,960
two variables are highly correlated with
each other.

41
00:02:03,180 --> 00:02:06,250
So here's an example, with, prostate
cancer data

42
00:02:06,250 --> 00:02:09,620
set and the elements of statistical
learning library.

43
00:02:09,620 --> 00:02:12,390
We can load the prostate data and look at

44
00:02:12,390 --> 00:02:16,620
that data set it has, 97 observations on
ten variables.

45
00:02:16,620 --> 00:02:19,245
And so one thing that we might want to be
able

46
00:02:19,245 --> 00:02:23,222
to do is make a prediction about, prostate
cancer, PSA.

47
00:02:23,222 --> 00:02:27,380
Based on a large number of predictors in
the data set.

48
00:02:27,380 --> 00:02:29,260
And so this is a very typical of

49
00:02:29,260 --> 00:02:31,450
what happens when you build these models
in practice.

50
00:02:31,450 --> 00:02:35,950
So suppose we predict with all possible
combinations of predictor variables.

51
00:02:35,950 --> 00:02:38,260
We go to the linear regression model.

52
00:02:38,260 --> 00:02:40,080
For the outcome where we build one

53
00:02:40,080 --> 00:02:44,280
regression model for every possible
combination of vectors.

54
00:02:44,280 --> 00:02:47,240
Then we can see as the number of
predictors increases from

55
00:02:47,240 --> 00:02:51,550
left to right here, the training set error
always goes down.

56
00:02:51,550 --> 00:02:52,500
It has to down.

57
00:02:52,500 --> 00:02:56,890
As you include more predictors, the
training set error will always decrease.

58
00:02:56,890 --> 00:03:00,080
But this is a typical pattern of what you
observe with real data.

59
00:03:00,080 --> 00:03:02,970
That the test set data on the other hand,
as the number

60
00:03:02,970 --> 00:03:06,770
of predictors increases, the test set
error goes down, which is good.

61
00:03:06,770 --> 00:03:11,590
But then eventually it hits a plateau, and
it starts to go back up again.

62
00:03:11,590 --> 00:03:14,430
This is because we're overfitting the data
in the training set, and

63
00:03:14,430 --> 00:03:17,890
eventually, we may not want to include so
many predictors in our model.

64
00:03:19,170 --> 00:03:21,240
This is an incredibly common pattern.

65
00:03:21,240 --> 00:03:25,371
Basically any measure of model complexity
on the x-axis versus

66
00:03:25,371 --> 00:03:29,663
the expected Residual Sum of Squares of
the predicted error.

67
00:03:29,663 --> 00:03:32,251
You can see that in the training set
almost always

68
00:03:32,251 --> 00:03:36,115
the error goes monotonically down, in
other words, as you build.

69
00:03:36,115 --> 00:03:40,700
More and more complicated models, the
training error will always decrease.

70
00:03:40,700 --> 00:03:43,109
But on a testing set, the error will

71
00:03:43,109 --> 00:03:46,333
decrease for a while, eventual hit a
minimum.

72
00:03:46,333 --> 00:03:49,296
And then, start to increase again as the

73
00:03:49,296 --> 00:03:52,707
model gets too complex and over fits the
data.

74
00:03:52,707 --> 00:03:57,269
So, in general, what the best approach
might be when, when you

75
00:03:57,269 --> 00:04:02,540
have enough data and you have enough
computation time to split samples.

76
00:04:02,540 --> 00:04:07,120
So the idea is divide your data into
training, test, and validation sets.

77
00:04:07,120 --> 00:04:09,490
You treat the validation set as the test

78
00:04:09,490 --> 00:04:12,720
data and you train every possible
competing model.

79
00:04:12,720 --> 00:04:15,400
So all possible subsets on the training
data.

80
00:04:15,400 --> 00:04:18,690
And pick the best, the one that works best
on the validation data set.

81
00:04:19,720 --> 00:04:22,540
But now that we've used the validation set
for training, we

82
00:04:22,540 --> 00:04:26,540
need to assess the error rate on a
completely independent set.

83
00:04:26,540 --> 00:04:30,080
So we appropriately assess this
performance by applying our

84
00:04:30,080 --> 00:04:32,210
prediction to the new data in the test
set.

85
00:04:33,670 --> 00:04:38,804
Sometimes you may re, re-perform the
splitting and analysis several times.

86
00:04:38,804 --> 00:04:40,964
In order to get a better average estimate
of

87
00:04:40,964 --> 00:04:43,064
what the out of sample error rate will be.

88
00:04:43,064 --> 00:04:44,746
But there are two common problems with
this

89
00:04:44,746 --> 00:04:47,040
approach, one is that there might be
limited data.

90
00:04:47,040 --> 00:04:50,200
Here we're breaking the data set up into
three different data sets.

91
00:04:50,200 --> 00:04:51,850
It might not be possible to get a very

92
00:04:51,850 --> 00:04:55,340
good model fit when we split the data that
finely.

93
00:04:55,340 --> 00:04:57,550
Second, is computational complexity.

94
00:04:57,550 --> 00:05:00,240
If you're trying all possible subsets of
models it can be very

95
00:05:00,240 --> 00:05:03,200
complicated, especially if you have a lot,
a lot of predictor variables.

96
00:05:04,810 --> 00:05:08,068
So another approach is to try to decompose
the prediction error.

97
00:05:08,068 --> 00:05:11,106
And see if there's another way that we can
work directly get

98
00:05:11,106 --> 00:05:15,690
at including only the variable that need
to be included in the model.

99
00:05:15,690 --> 00:05:19,030
So if we assume that the variable y can be

100
00:05:19,030 --> 00:05:22,660
predicted as a function of x, plus some
error term,.

101
00:05:22,660 --> 00:05:26,573
Then the expected prediction error is the
expected difference

102
00:05:26,573 --> 00:05:30,571
between the outcome and the prediction of
the outcome squared.

103
00:05:30,571 --> 00:05:35,400
And so, f-hat lambda here is the estimate
from the training set.

104
00:05:35,400 --> 00:05:37,800
Using a particular set of tuning
parameters lambda.

105
00:05:38,930 --> 00:05:40,210
Then if we look at a new point.

106
00:05:40,210 --> 00:05:42,010
So we bring in a new data point and we

107
00:05:42,010 --> 00:05:47,120
look at the distance between our var, our
observed outcome.

108
00:05:47,120 --> 00:05:49,990
And the prediction on the new data point.

109
00:05:49,990 --> 00:05:54,010
That can be decomposed after some algebra
into irreducible error.

110
00:05:54,010 --> 00:05:55,680
This sigma squared.

111
00:05:55,680 --> 00:05:59,130
The bias, so this is the difference
between our expected

112
00:05:59,130 --> 00:06:03,700
prediction and the truth, and the variance
of our estimate.

113
00:06:03,700 --> 00:06:06,030
So the question, so whenever you're
building a prediction

114
00:06:06,030 --> 00:06:08,810
model the goal is to reduce this overall
quantity.

115
00:06:08,810 --> 00:06:11,390
So this is basically the expected.

116
00:06:11,390 --> 00:06:17,520
Mean squared error between our our outcome
and our prediction.

117
00:06:17,520 --> 00:06:21,360
And so, the irreducible error can't
usually be reduced.

118
00:06:21,360 --> 00:06:25,700
It's value that's just part of the data
you're collecting.

119
00:06:25,700 --> 00:06:27,860
But you can trade off bias and variance.

120
00:06:27,860 --> 00:06:31,020
And that's what the idea behind
regularized regression does.

121
00:06:32,630 --> 00:06:35,960
So another issue for high dimensional
data, is suppose I'm just showing you

122
00:06:35,960 --> 00:06:38,990
a simple example of what happens when you
have a lot of predictors.

123
00:06:38,990 --> 00:06:42,470
So here I'm sub setting just a small
subset of the prostate

124
00:06:42,470 --> 00:06:46,620
data, so imagine that I only had five
observations in my training set.

125
00:06:46,620 --> 00:06:48,600
It has more than five predictor variables.

126
00:06:48,600 --> 00:06:50,560
So I fit a linear model relating

127
00:06:50,560 --> 00:06:52,960
the outcome to all of these predictor
variables.

128
00:06:52,960 --> 00:06:55,000
And there are more than five.

129
00:06:55,000 --> 00:06:57,320
Then some of them will get estimates.

130
00:06:57,320 --> 00:06:59,450
But some of them will be NA.

131
00:06:59,450 --> 00:07:01,520
In other words, r won't be able to
estimate

132
00:07:01,520 --> 00:07:04,690
them because you have more predictors than
you have samples.

133
00:07:04,690 --> 00:07:10,890
And, so you have, design matrix that
cannot be inverted.

134
00:07:13,040 --> 00:07:16,457
So here we have one approach to dealing
with this problem is

135
00:07:16,457 --> 00:07:19,750
we can, and the other problem of trying to
select our model.

136
00:07:19,750 --> 00:07:22,324
Is we could take this model that we have,

137
00:07:22,324 --> 00:07:25,442
and assume that it has a linear form, like
this.

138
00:07:25,442 --> 00:07:29,660
So assume that it's a linear regression
model, like we've talked about before.

139
00:07:29,660 --> 00:07:34,050
And then constrain only the lambda of the
coefficients to be non-zero.

140
00:07:34,050 --> 00:07:36,120
And then the question is, after we pick

141
00:07:36,120 --> 00:07:39,718
lambda, so suppose there are only three
non-zero coefficients.

142
00:07:39,718 --> 00:07:42,500
Then we have to try all the possible
combinations of

143
00:07:42,500 --> 00:07:46,330
three coefficients that are non-zero, and
then fit the best model.

144
00:07:46,330 --> 00:07:49,600
So that's still computationally quite
demanding.

145
00:07:49,600 --> 00:07:52,810
So another approach is to use regularized
regression.

146
00:07:52,810 --> 00:07:54,812
So if the beta j, so if our coefficients

147
00:07:54,812 --> 00:07:57,922
that we're fitting in the linear model are
unconstrained.

148
00:07:57,922 --> 00:08:02,381
In other words, we don't make, we don't
claim that they have any particular form.

149
00:08:02,381 --> 00:08:04,610
They may explode if they're very, if you
have

150
00:08:04,610 --> 00:08:08,050
very highly correlated variables that
you're using for prediction.

151
00:08:08,050 --> 00:08:10,530
And so, they can be susceptible to high
variance.

152
00:08:10,530 --> 00:08:14,080
And the high variance means that you'll
get predictions that aren't as accurate.

153
00:08:15,100 --> 00:08:18,410
So to control the variance we might
regularize, or shrink the coefficients.

154
00:08:18,410 --> 00:08:22,440
So, remember that, we, what we might
want to minimize, is some

155
00:08:22,440 --> 00:08:26,800
kind of distance between our outcome that
we have and our linear model.

156
00:08:26,800 --> 00:08:31,770
So here, this is the distance between the
outcome and the linear model fit squared.

157
00:08:31,770 --> 00:08:33,870
That's the residual sum of squares.

158
00:08:33,870 --> 00:08:38,510
Then you might also add a penalty term
here.

159
00:08:38,510 --> 00:08:41,379
That says, the penalty will basically say:
if the beta

160
00:08:41,379 --> 00:08:45,260
coefficients are too big, it will shrink
them back down.

161
00:08:45,260 --> 00:08:48,840
So the penalty is usually used to reduce
complexity.

162
00:08:48,840 --> 00:08:51,030
It can be used to reduce variance.

163
00:08:51,030 --> 00:08:53,100
And it can respect some of the structure
in the

164
00:08:53,100 --> 00:08:55,110
problem if you set the penalty up in the
right way.

165
00:08:55,110 --> 00:08:58,015
The first approach that was used in this

166
00:08:58,015 --> 00:09:02,920
sort of penalized regression is to fit the
regression model.

167
00:09:02,920 --> 00:09:06,730
Here again, we're penalizing a distance
between

168
00:09:06,730 --> 00:09:10,120
our outcome y and our regression model
here.

169
00:09:10,120 --> 00:09:12,680
And then we also have a term here.

170
00:09:12,680 --> 00:09:16,010
That is lambda times the sum of the beta
j's squared.

171
00:09:16,010 --> 00:09:17,100
So, what does this mean?

172
00:09:17,100 --> 00:09:20,474
If the beta j's squares are really big
then this term will

173
00:09:20,474 --> 00:09:24,300
get too big, so we'll get, we won't get a
very good fit.

174
00:09:24,300 --> 00:09:26,892
This whole quantity will end up being very
big, so

175
00:09:26,892 --> 00:09:30,860
it basically requires that some of the
beta j's be small.

176
00:09:30,860 --> 00:09:35,136
It's actually equivalent to solving this
problem where we're looking for

177
00:09:35,136 --> 00:09:39,088
the smallest sum of squared here and sum
of squared differences here.

178
00:09:39,088 --> 00:09:43,333
Subject to a particular constraint that,
the sum

179
00:09:43,333 --> 00:09:46,775
of squared beta j's is less than s.

180
00:09:46,775 --> 00:09:49,548
So the idea here is that the inclusion of

181
00:09:49,548 --> 00:09:54,393
this lambda coefficient may also even make
the problem non-singular.

182
00:09:54,393 --> 00:09:57,370
Even when the x transpose x is not
invertible.

183
00:09:57,370 --> 00:10:01,800
In other words, in that model fit where we
have more predictors

184
00:10:01,800 --> 00:10:06,640
than we do observations the ridge
regression model can still be fit.

185
00:10:08,360 --> 00:10:10,780
So this is what the coefficient path looks
like.

186
00:10:10,780 --> 00:10:13,140
So what do I mean by coefficient path.

187
00:10:13,140 --> 00:10:16,233
For every different choice of lamda, that
penalized

188
00:10:16,233 --> 00:10:20,020
regression problem on the previous page,
as gambit increases.

189
00:10:20,020 --> 00:10:23,840
That means that we penalize the big datas
more and more.

190
00:10:23,840 --> 00:10:26,382
So we start off with the betas being equal
to

191
00:10:26,382 --> 00:10:29,500
a certain of values here when lambda's
equal to 0.

192
00:10:29,500 --> 00:10:32,580
That's just a standard linear with
regression values.

193
00:10:32,580 --> 00:10:36,238
And as you increase lambda, all of the
coefficients get closer to 0.

194
00:10:36,238 --> 00:10:40,380
Because we're penalizing the coefficients,
and make, and making them smaller.

195
00:10:43,030 --> 00:10:47,030
So the tuning parameter lambda controls
the size of the control coefficients.

196
00:10:47,030 --> 00:10:49,630
Lambda controls the amount of
regularization.

197
00:10:49,630 --> 00:10:52,560
As lambda get's closer and closer to 0, we
basically go back to

198
00:10:52,560 --> 00:10:56,810
the least square solution which is what
you get from a standard linear model.

199
00:10:56,810 --> 00:10:59,260
And as lambda goes to infinitely we
basically,

200
00:10:59,260 --> 00:11:02,340
so in other words as lambda get's really
big.

201
00:11:02,340 --> 00:11:05,286
It penalizes the coefficients a lot, and
so all of the

202
00:11:05,286 --> 00:11:10,660
conditioned coefficients go toward 0 as
the, tuning parameter gets really big.

203
00:11:10,660 --> 00:11:13,879
So taking that parameter can be done with
cross-validation or

204
00:11:13,879 --> 00:11:17,108
other techniques to try to pick the
optimal tuning parameter.

205
00:11:17,108 --> 00:11:18,890
That trades off bias for variance.

206
00:11:20,370 --> 00:11:23,680
A similar approach can be done with a
slight change of penalty.

207
00:11:23,680 --> 00:11:27,772
So again, here we might be solving the
problem again, this lees squares problem.

208
00:11:27,772 --> 00:11:30,394
This is the standard trying to identify
the beta

209
00:11:30,394 --> 00:11:34,220
values that make this distance to the
outcome smallest.

210
00:11:34,220 --> 00:11:36,430
And here we can constrain it subject to
the sum of

211
00:11:36,430 --> 00:11:39,140
the absolute value of the beta j's being
less than sum value.

212
00:11:39,140 --> 00:11:43,270
You can also write that as a penalized
regression of this

213
00:11:43,270 --> 00:11:47,310
form, so we're trying to solve this
penalized sum of squares.

214
00:11:47,310 --> 00:11:49,790
So for ortho normal design matrix which
you

215
00:11:49,790 --> 00:11:53,520
can, see on Wikipedia or the normal design
matrix.

216
00:11:53,520 --> 00:11:57,510
The idea is that this actually has a
closed form solution.

217
00:11:57,510 --> 00:12:02,241
And the closed form solution is basically,
take the absolute

218
00:12:02,241 --> 00:12:06,380
value of the beta j and subtract off a
gamma value.

219
00:12:06,380 --> 00:12:08,140
And take only the positive part.

220
00:12:08,140 --> 00:12:10,769
In other words if gamma is bigger than
your least

221
00:12:10,769 --> 00:12:13,801
squared beta hat j then this will be a
negative number.

222
00:12:13,801 --> 00:12:18,280
And you're taking only the positive part
so you set it equal to 0.

223
00:12:18,280 --> 00:12:21,390
So if it's a positive though if this
absolute

224
00:12:21,390 --> 00:12:25,450
beta hat j is bigger than, the gamma
value.

225
00:12:25,450 --> 00:12:28,257
Then this whole number will be a smaller
positive number.

226
00:12:28,257 --> 00:12:30,129
It will be shrunk by the amount gamma.

227
00:12:30,129 --> 00:12:34,210
And then, we multiply it by the sign of
the original coefficient.

228
00:12:34,210 --> 00:12:34,950
So what is this doing?

229
00:12:34,950 --> 00:12:37,751
Its basically saying the lasso shrinks all
of the

230
00:12:37,751 --> 00:12:40,559
coefficients and set some of them to
exactly 0.

231
00:12:40,559 --> 00:12:44,954
And some people like this approach because
it both shrinks coefficients.

232
00:12:44,954 --> 00:12:47,106
And by setting something exactly to 0

233
00:12:47,106 --> 00:12:49,870
it performs model selection for you in
advance.

234
00:12:51,440 --> 00:12:55,140
There are really good set of lecture notes
from Hector Corrada Bravo.

235
00:12:55,140 --> 00:12:57,830
That you can find here at this link.

236
00:12:57,830 --> 00:13:02,220
He also has a very nice list on the large
number of penalized regression models.

237
00:13:02,220 --> 00:13:04,440
And in the Elements of Statistical
Learning book

238
00:13:04,440 --> 00:13:08,430
covers this penalized regression idea a
quite extensive detail.

239
00:13:08,430 --> 00:13:10,390
If you want to follow along there.

240
00:13:10,390 --> 00:13:13,260
In caret, if you want to fit these models,
you can set the

241
00:13:13,260 --> 00:13:16,450
method to ridge, lasso or relaxo to

242
00:13:16,450 --> 00:13:18,970
fit different kinds of penalized
regression models.

