1
00:00:00,470 --> 00:00:03,750
This lecture is about predicting with
trees.

2
00:00:03,750 --> 00:00:05,169
The basic idea is that if you have a

3
00:00:05,169 --> 00:00:07,250
bunch of variables that you want to use to
predict

4
00:00:07,250 --> 00:00:09,432
an outcome, you can take each of those
variables,

5
00:00:09,432 --> 00:00:12,580
and use it to split the outcome into
different groups.

6
00:00:12,580 --> 00:00:15,522
And, so as you split the outcomes into
different groups, then

7
00:00:15,522 --> 00:00:19,330
you can evaluate the homogeneity of the
outcome within each group.

8
00:00:19,330 --> 00:00:21,329
And continue to split again if necessary,
and

9
00:00:21,329 --> 00:00:23,432
then, until you get outcomes that are
separated

10
00:00:23,432 --> 00:00:25,586
into groups that are homogeneous enough,
or that

11
00:00:25,586 --> 00:00:27,500
they are small enough that you need to
stop.

12
00:00:27,500 --> 00:00:29,563
The pros of this approach are that it's
easy

13
00:00:29,563 --> 00:00:31,736
to interpret right, and you tend to get
better

14
00:00:31,736 --> 00:00:34,413
performance in non-linear settings than
you do with the

15
00:00:34,413 --> 00:00:37,670
linear regression models we talked about
in the previous lectures.

16
00:00:38,790 --> 00:00:40,510
The cons are that without precluding or
some

17
00:00:40,510 --> 00:00:44,030
kind of cross-validation, this can lead to
overfitting.

18
00:00:44,030 --> 00:00:46,470
And they can be harder to estimate
uncertainty than

19
00:00:46,470 --> 00:00:49,610
it can be for the linear regression model
setting.

20
00:00:49,610 --> 00:00:52,048
In general the results may be variable
depending on exact

21
00:00:52,048 --> 00:00:54,860
values of the parameters, or the variables
that you collected.

22
00:00:56,450 --> 00:00:57,840
So here's an example of what a decision

23
00:00:57,840 --> 00:00:59,630
tree looks like after it has already been
built.

24
00:00:59,630 --> 00:01:03,454
So this is a decision tree that appeared
in the New York Times during the 2008

25
00:01:03,454 --> 00:01:06,195
elections, when Bar, Barack Obama was
running against

26
00:01:06,195 --> 00:01:10,130
Hillary Clinton for the Democratic
nomination for President.

27
00:01:10,130 --> 00:01:12,960
And they were trying to decide what would
be a prediction rule

28
00:01:12,960 --> 00:01:16,230
for whether a county would vote for Obama
or for Hillary Clinton.

29
00:01:17,710 --> 00:01:21,177
And, so the way that happened was that
they would, build a prediction model that

30
00:01:21,177 --> 00:01:22,959
would ask questions of each of the
different

31
00:01:22,959 --> 00:01:25,370
variables that they had in their data set.

32
00:01:25,370 --> 00:01:27,400
So the best split happened to be this
variable.

33
00:01:27,400 --> 00:01:30,842
So if the county was more than 20% African
American, then

34
00:01:30,842 --> 00:01:35,340
the county was much more likely to vote
for, Barack Obama.

35
00:01:35,340 --> 00:01:38,320
If it was less than 20% African American,
then it

36
00:01:38,320 --> 00:01:41,780
became more likely that the county would
vote for Hilary Clinton.

37
00:01:41,780 --> 00:01:43,990
Then within each of these two subgroups,
they went and

38
00:01:43,990 --> 00:01:47,310
looked for other variables that might
split those subgroups out.

39
00:01:47,310 --> 00:01:50,340
So, on this branch here, the next question
was,

40
00:01:50,340 --> 00:01:53,840
is the high school graduation rate greater
than 78%?

41
00:01:53,840 --> 00:01:57,580
If it wasn't, then the county was more
likely to vote for Hillary

42
00:01:57,580 --> 00:02:02,630
Clinton, and if it was, then it's more
likely to vote for Barack Obama.

43
00:02:02,630 --> 00:02:06,477
And the algorithm continues in that manner
until you've split out

44
00:02:06,477 --> 00:02:11,970
into smallest, the smallest subgroups that
you are willing to consider.

45
00:02:11,970 --> 00:02:14,180
And, so you see that within each of these

46
00:02:14,180 --> 00:02:17,900
leaves of the tree, the predictions were
quite homogeneous.

47
00:02:17,900 --> 00:02:18,900
In other words.

48
00:02:18,900 --> 00:02:25,710
Obama was 383 out of about 450 counties in
this case.

49
00:02:25,710 --> 00:02:32,302
And Hilary Clinton won 704 out of about
790 or 800 counties in this ca, case.

50
00:02:32,302 --> 00:02:34,670
In other words, these questions split the

51
00:02:34,670 --> 00:02:37,698
counties into groups that were homogenous
within each

52
00:02:37,698 --> 00:02:40,198
leaf their prediction was likely to come
true

53
00:02:40,198 --> 00:02:43,250
that, that candidate would win in the
election.

54
00:02:44,840 --> 00:02:47,000
So the basic algorithm behind building one
of these trees

55
00:02:47,000 --> 00:02:49,720
is, start with all the variables of one
big group.

56
00:02:49,720 --> 00:02:51,450
And then, you find the first variable that

57
00:02:51,450 --> 00:02:54,710
best splits the outcomes into two
different homogenous groups.

58
00:02:55,800 --> 00:02:59,320
You then divided the data into two groups
which we call leaves,

59
00:02:59,320 --> 00:03:02,730
and on, and the split that you just
performed is called a node.

60
00:03:03,980 --> 00:03:07,390
Within each split, we then search through
all the variables, again.

61
00:03:07,390 --> 00:03:09,470
Including the variable we just split on.

62
00:03:09,470 --> 00:03:12,458
And try to find within that group if
there's another variable

63
00:03:12,458 --> 00:03:17,000
or split that separates the outcome, into
even more homogeneous groups.

64
00:03:17,000 --> 00:03:20,220
You continue until the groups are too
small, or they're sufficiently pure.

65
00:03:20,220 --> 00:03:22,480
In other words, sufficiently homogeneous.

66
00:03:22,480 --> 00:03:23,760
To, stop the algorithm.

67
00:03:23,760 --> 00:03:27,060
So there are different measures of
impurity.

68
00:03:27,060 --> 00:03:31,150
And they're all based on basically this
probability that you can estimate.

69
00:03:31,150 --> 00:03:32,450
So, within a particular group.

70
00:03:32,450 --> 00:03:37,891
So, in the le, m, m leaf then you have n
total objects that you might consider,

71
00:03:37,891 --> 00:03:44,100
and you can count the number of times that
a particular class appears in that leaf.

72
00:03:44,100 --> 00:03:48,210
So this is the number of times that class
k appears in leaf m.

73
00:03:48,210 --> 00:03:50,340
So that's this probability.

74
00:03:50,340 --> 00:03:54,410
So it's p hat for the mth leaf and the kth
class.

75
00:03:54,410 --> 00:03:58,610
The misclassification error is 1 minus the
probability that you're

76
00:03:58,610 --> 00:04:02,790
equal to the most common class in that
particular leaf.

77
00:04:02,790 --> 00:04:06,250
So for example, if you're if it's a leaf
where.

78
00:04:06,250 --> 00:04:07,820
Almost all of the counties would vote for

79
00:04:07,820 --> 00:04:11,550
Barack Obama, then 1 minus the
misclassification error is

80
00:04:11,550 --> 00:04:14,390
1 minus the er the misclassification error
is 1

81
00:04:14,390 --> 00:04:17,130
minus the probability that you'd vote for
Barack Obama.

82
00:04:18,140 --> 00:04:19,890
So zero means perfect purity.

83
00:04:19,890 --> 00:04:23,140
In other words, there's no
misclassification error and

84
00:04:23,140 --> 00:04:25,760
all of the counties would go for Barack
Obama.

85
00:04:25,760 --> 00:04:29,318
0.5 means no purity because, it, it's not
one,

86
00:04:29,318 --> 00:04:33,821
because in any particular leaf if the,
counties was overwhelming,

87
00:04:33,821 --> 00:04:37,889
the counties where overwhelming likely to
vote for Hilary Clinton

88
00:04:37,889 --> 00:04:41,610
then you would get perfect purity in the
other direction.

89
00:04:41,610 --> 00:04:45,058
So it's actually when the leaf is
perfectly balanced between the two

90
00:04:45,058 --> 00:04:47,377
different outcomes, that's when you don't

91
00:04:47,377 --> 00:04:50,060
have homogeneity within that particular
leaf.

92
00:04:51,160 --> 00:04:54,270
Similarly there's something called the
Gini index which is

93
00:04:54,270 --> 00:04:57,290
not to be confused with the Gini
coefficient in economics,.

94
00:04:57,290 --> 00:05:00,719
And it's basically 1 minus the sum of the
squared

95
00:05:00,719 --> 00:05:05,140
probabilities that you belong to any of
the different classes.

96
00:05:05,140 --> 00:05:06,960
So again zero means perfect purity.

97
00:05:06,960 --> 00:05:12,230
In other words the class has one
particular class has probability equal

98
00:05:12,230 --> 00:05:15,505
to 1 and all the other classes have
probability equal to 0.

99
00:05:15,505 --> 00:05:16,480
0.5 means no purity.

100
00:05:16,480 --> 00:05:23,570
In other words, all of the classes are
perfectly balanced within each leaf.

101
00:05:23,570 --> 00:05:29,410
Deviance and information gain is another
measure that can be used, and so, it's

102
00:05:29,410 --> 00:05:35,910
called deviance if you use log with base e
here, and log base 2 is information gain.

103
00:05:35,910 --> 00:05:39,340
And it's basically minus the sum of the
probability.

104
00:05:39,340 --> 00:05:43,311
Of being assigned to class k and leaf m
times

105
00:05:43,311 --> 00:05:47,690
the log base 2 or base e, of that same
probability.

106
00:05:48,776 --> 00:05:50,937
Value of zero means there is perfect
purity within

107
00:05:50,937 --> 00:05:53,260
the leaf, and value of one means there's
no purity.

108
00:05:53,260 --> 00:05:57,783
If you go to this link from Wikipedia,
you'll have a lot more information about

109
00:05:57,783 --> 00:06:02,710
how these measures of impurity are used to
separate the, the values within each leaf.

110
00:06:03,810 --> 00:06:07,350
So here's what they look like for a
couple, for an s, for a specific example.

111
00:06:07,350 --> 00:06:10,030
So suppose we had a variable that was
trying to

112
00:06:10,030 --> 00:06:12,930
split the dots into the blue and the red
dots.

113
00:06:12,930 --> 00:06:16,510
If the variable splitting in the following
way were 15 of the dots were blue

114
00:06:16,510 --> 00:06:22,590
in a particular leaf and only one was red,
that would be a relatively pure situation.

115
00:06:22,590 --> 00:06:25,640
And so, misclassification rate would be
one out

116
00:06:25,640 --> 00:06:28,300
of 16, so that's just this one dot here.

117
00:06:28,300 --> 00:06:32,382
And so it's relatively low value the GinI
coefficient would also

118
00:06:32,382 --> 00:06:35,772
be low, because it's one minus 1 divided
by 16 squared

119
00:06:35,772 --> 00:06:39,856
plus 15 divided 16 squared which is a
relatively small number

120
00:06:39,856 --> 00:06:44,860
and then the information gained is
similarly would be, in this case.

121
00:06:44,860 --> 00:06:50,481
&Uh, smaller towards 0, because it's 1
over 16 times log base 2,

122
00:06:50,481 --> 00:06:55,854
1 over 16 plus 15 divided by 16 times log
base 2 15 over 16.

123
00:06:55,854 --> 00:07:00,140
So that's the case where there is
relatively pure outcome in the two groups.

124
00:07:00,140 --> 00:07:02,060
We can also look at the case where it's
not.

125
00:07:02,060 --> 00:07:07,530
So suppose this variable split the
outcomes into a leaf where half

126
00:07:07,530 --> 00:07:10,720
of the values were blue, and half of the
values were red.

127
00:07:10,720 --> 00:07:14,720
So that wouldn't be a very good split,
because it's basically a

128
00:07:14,720 --> 00:07:17,760
coin flip whether you're from one class to
the other in that leaf.

129
00:07:17,760 --> 00:07:21,120
And so then this classification right here
is .5, it's very large.

130
00:07:21,120 --> 00:07:23,960
The Gini index is also maximized, it's .5,

131
00:07:23,960 --> 00:07:27,290
and the information is also large, it's
one.

132
00:07:27,290 --> 00:07:29,819
So, this would be a split that wouldn't be
made, because

133
00:07:29,819 --> 00:07:32,760
it wouldn't separate the two groups very
well on that particular leaf.

134
00:07:34,460 --> 00:07:36,530
So I'm just going to show you an example
with the iris data.

135
00:07:36,530 --> 00:07:39,970
This is a very old data set, but it shows
you the idea of how this works.

136
00:07:40,970 --> 00:07:43,700
So, this, I'm loading the data with the
command data iris.

137
00:07:43,700 --> 00:07:47,380
And then I'm loading the ggplot2 library
for making plots.

138
00:07:47,380 --> 00:07:48,891
So the names of this data set are the

139
00:07:48,891 --> 00:07:52,030
different variables that we're going to be
using to predict with.

140
00:07:52,030 --> 00:07:56,554
Here it's sepal length, Sepal.Width,
Petal.Length and Petal.Width

141
00:07:56,554 --> 00:08:00,190
and what we're trying to predict is the
Species.

142
00:08:00,190 --> 00:08:02,990
So there's 50 of these three different
species

143
00:08:02,990 --> 00:08:04,780
that we're trying to predict with those
variables.

144
00:08:06,330 --> 00:08:10,256
So again I always separate the data into
the training and the test set, and in this

145
00:08:10,256 --> 00:08:12,335
case I have 45 examples in my training set

146
00:08:12,335 --> 00:08:14,440
that I can use to build a prediction
model.

147
00:08:15,950 --> 00:08:17,593
And the first thing that I do is, I'm

148
00:08:17,593 --> 00:08:20,380
going to plot the petal width versus the
sepal width.

149
00:08:20,380 --> 00:08:24,380
So that's petal width on the x axis, the
sepal width on the y axis, and then I'm

150
00:08:24,380 --> 00:08:26,830
going to color it by the different
species, and you

151
00:08:26,830 --> 00:08:29,550
can see there are three very distinct
clusters here.

152
00:08:30,840 --> 00:08:33,950
So, it's a relatively easy classification
problem.

153
00:08:33,950 --> 00:08:36,798
It might be a little bit challenging for a
linear model,

154
00:08:36,798 --> 00:08:39,162
but not necessarily challenging for this

155
00:08:39,162 --> 00:08:41,960
more advanced model with classification
trace.

156
00:08:43,710 --> 00:08:47,820
So you can fit the model using the train
function in caret.

157
00:08:47,820 --> 00:08:49,710
So again, I'm training the model.

158
00:08:49,710 --> 00:08:51,350
The outcome is species.

159
00:08:51,350 --> 00:08:53,650
I'm predicting with all the remaining
variables, which is

160
00:08:53,650 --> 00:08:55,820
why I have this tilde and then a dot.

161
00:08:55,820 --> 00:08:58,796
And I'm telling it the method is rpart
which is [UNKNOWN]

162
00:08:58,796 --> 00:09:03,500
package for doing regression and
classification trees, one of the packages.

163
00:09:03,500 --> 00:09:05,930
And I'm telling you to use the training
data.

164
00:09:05,930 --> 00:09:10,000
If I look at the final model, it tells me
what all the nodes are and how they're

165
00:09:10,000 --> 00:09:15,110
split and in order to, and what the the

166
00:09:15,110 --> 00:09:17,690
probability of being in each class is for
each split.

167
00:09:18,760 --> 00:09:23,899
So, here for example, there's a split that
says petal.length is less

168
00:09:23,899 --> 00:09:28,869
than 2.45, and if that happens then you in
that case all of the

169
00:09:28,869 --> 00:09:34,610
examples that have pedal length less than
2.45 belong to this bc setosa.

170
00:09:34,610 --> 00:09:37,002
So, you can read those model splits to

171
00:09:37,002 --> 00:09:40,130
tell you, what the classification tree is
doing.

172
00:09:40,130 --> 00:09:44,270
You can also make a plot of the
classification tree.

173
00:09:44,270 --> 00:09:47,916
And so if you just, plot the final model
that's produce what

174
00:09:47,916 --> 00:09:51,960
it will do is it will produce what's
called a dandergram, like this.

175
00:09:51,960 --> 00:09:54,275
And so it's a little hard to read, it's
cut

176
00:09:54,275 --> 00:09:57,620
off here, but you can see petal length
less than 2.45.

177
00:09:57,620 --> 00:09:59,030
Then you're assigned a setosa.

178
00:09:59,030 --> 00:10:02,850
And so you can follow to the left what
happens if the petal length is

179
00:10:02,850 --> 00:10:05,281
less than 2.45, and to the right what

180
00:10:05,281 --> 00:10:08,380
happens if petal length isn't less than
2.45.

181
00:10:08,380 --> 00:10:11,760
And then follow the next split here at
this node

182
00:10:11,760 --> 00:10:17,220
down to the, see the total classification
for any particular example.

183
00:10:17,220 --> 00:10:20,700
A prettier version of that plot can be
made with the rattle package.

184
00:10:20,700 --> 00:10:23,556
So, you use the function fancyRpartPlot,
and you pass

185
00:10:23,556 --> 00:10:26,510
it the final model that was fit using
caret.

186
00:10:26,510 --> 00:10:28,714
And it makes again dentigram, but now it's
a

187
00:10:28,714 --> 00:10:31,170
little bit easier to see, so here we see.

188
00:10:31,170 --> 00:10:34,793
That if the petal length is less than 2.5,
we move over here to the

189
00:10:34,793 --> 00:10:39,410
left, and if it's greater than 2.5 then we
go over here to the right.

190
00:10:39,410 --> 00:10:42,540
And then, within that split, so once
you've already made that

191
00:10:42,540 --> 00:10:44,938
decision about those samples, and if the
petal length is less

192
00:10:44,938 --> 00:10:47,600
that 4.8 you go down here to the left, and
if

193
00:10:47,600 --> 00:10:50,430
the petal length is greater you go over
here to the right.

194
00:10:50,430 --> 00:10:52,979
And so this makes it a little bit easier
to see what's going on.

195
00:10:54,020 --> 00:10:56,229
You can predict new values using the
predict function,

196
00:10:56,229 --> 00:10:59,110
just like you can with the other linear
regression models.

197
00:10:59,110 --> 00:11:02,048
Here the prediction is going to be a
particular class label,

198
00:11:02,048 --> 00:11:06,380
because the classification tree was built
to predict a particular class.

199
00:11:06,380 --> 00:11:10,480
And, so when I pass the new data, testing
data, to the model.

200
00:11:10,480 --> 00:11:14,637
It'll actually write out the different
categories of species,

201
00:11:14,637 --> 00:11:18,520
because it's actually predicting the class
for each variable.

202
00:11:19,830 --> 00:11:22,783
So notes and further resources,
classification trees

203
00:11:22,783 --> 00:11:25,421
are non-linear models, so they immediately
use

204
00:11:25,421 --> 00:11:28,374
interactions between variables, this is
important to

205
00:11:28,374 --> 00:11:30,319
keep in mind, because it's always a

206
00:11:30,319 --> 00:11:32,895
model that's built on the relationship
between

207
00:11:32,895 --> 00:11:35,028
multiple variables and if you have a

208
00:11:35,028 --> 00:11:37,163
large number of classes for a particular

209
00:11:37,163 --> 00:11:40,800
variable for example that you're
predicting with.

210
00:11:40,800 --> 00:11:42,340
The models can over-fit a little bit.

211
00:11:42,340 --> 00:11:45,912
The data transformations may be a little
bit less important.

212
00:11:45,912 --> 00:11:48,013
If you do any monotone transformation of

213
00:11:48,013 --> 00:11:51,227
a continuous variable, in other words, any
transformation

214
00:11:51,227 --> 00:11:53,142
that doesn't change the order of the

215
00:11:53,142 --> 00:11:56,560
values, but maybe makes them bigger or
smaller.

216
00:11:56,560 --> 00:11:58,650
Then you will get the same data

217
00:11:58,650 --> 00:12:01,460
splits, with the classification or
regression trees.

218
00:12:01,460 --> 00:12:03,720
This can make the transformations a little
bit less important.

219
00:12:03,720 --> 00:12:06,500
They can also be used for regression
problems.

220
00:12:06,500 --> 00:12:10,159
So, I showed measures of misclassifcation
impurity, you can also

221
00:12:10,159 --> 00:12:13,680
use root mean squared error as a measure
of impurity.

222
00:12:13,680 --> 00:12:16,600
And do a similar classification tree
building

223
00:12:16,600 --> 00:12:19,970
procedure for building models for
regression as well.

224
00:12:21,050 --> 00:12:23,870
There are multiple tree building options
in RR, both in

225
00:12:23,870 --> 00:12:27,980
the caret package, using the party
package, the rpart package.

226
00:12:27,980 --> 00:12:30,010
Or you can even use this package tree

227
00:12:30,010 --> 00:12:32,460
which doesn't appear in the caret package,
but

228
00:12:32,460 --> 00:12:33,951
I also has a lot of use, useful

229
00:12:33,951 --> 00:12:37,070
functions that can be used when building
these models.

230
00:12:37,070 --> 00:12:40,150
For more information you can read in, in
these two books,

231
00:12:40,150 --> 00:12:44,880
where there's a lot of good information
about classification and regression trees.

232
00:12:44,880 --> 00:12:46,600
Or, in this book about, that's more

233
00:12:46,600 --> 00:12:49,520
specifically targeted to this particular
prediction algorithm.

